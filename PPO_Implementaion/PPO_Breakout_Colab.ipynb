{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# PPO for Atari Breakout - Google Colab (GPU)\n",
                "\n",
                "This notebook trains a PPO agent on Breakout with automatic video saving to Google Drive.\n",
                "\n",
                "**Setup:**\n",
                "1. Runtime → Change runtime type → Hardware accelerator → **GPU** (T4 recommended)\n",
                "2. Run all cells in order\n",
                "3. Videos will be automatically saved to Google Drive"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "mount_drive"
            },
            "source": [
                "## 1. Mount Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mount_drive_code"
            },
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "DRIVE_PATH = '/content/drive/MyDrive/PPO_Breakout'\n",
                "os.makedirs(f'{DRIVE_PATH}/videos', exist_ok=True)\n",
                "os.makedirs(f'{DRIVE_PATH}/models', exist_ok=True)\n",
                "\n",
                "print(f\"✅ Google Drive mounted at: {DRIVE_PATH}\")\n",
                "print(f\"   Videos: {DRIVE_PATH}/videos\")\n",
                "print(f\"   Models: {DRIVE_PATH}/models\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "install"
            },
            "source": [
                "## 2. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_code"
            },
            "outputs": [],
            "source": [
                "!pip install -q gymnasium[atari] ale-py torch torchvision matplotlib tqdm\n",
                "!pip install -q \"gymnasium[accept-rom-license]\"\n",
                "\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"GPU: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "code"
            },
            "source": [
                "## 3. PPO Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "code_full"
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.distributions import Categorical\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "import gymnasium as gym\n",
                "import ale_py\n",
                "from gymnasium.wrappers import *\n",
                "import shutil\n",
                "from IPython.display import clear_output\n",
                "\n",
                "gym.register_envs(ale_py)\n",
                "\n",
                "try:\n",
                "    from gymnasium.wrappers import GrayScaleObservation as GrayscaleObservation\n",
                "except:\n",
                "    from gymnasium.wrappers import GrayscaleObservation\n",
                "\n",
                "class NN(nn.Module):\n",
                "    def __init__(self, input_size, hidden_layers, output_size, act_fun=nn.ReLU):\n",
                "        super().__init__()\n",
                "        layers = []\n",
                "        _in = input_size\n",
                "        for h in hidden_layers:\n",
                "            layers.append(nn.Linear(_in, h))\n",
                "            layers.append(act_fun())\n",
                "            _in = h\n",
                "        layers.append(nn.Linear(_in, output_size))\n",
                "        self.model = nn.Sequential(*layers)\n",
                "    def forward(self, x):\n",
                "        return self.model(x)\n",
                "\n",
                "class ActorCritic(nn.Module):\n",
                "    def __init__(self, hidden_layers, output_dim, input_shape=(4, 84, 84)):\n",
                "        super().__init__()\n",
                "        self.conv = nn.Sequential(\n",
                "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), nn.ReLU(),\n",
                "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
                "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
                "        )\n",
                "        with torch.no_grad():\n",
                "            flatten_size = self.conv(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
                "        self.actor = NN(flatten_size, hidden_layers, output_dim)\n",
                "        self.critic = NN(flatten_size, hidden_layers, 1)\n",
                "    def forward(self, state):\n",
                "        x = self.conv(state).view(state.size(0), -1)\n",
                "        return self.actor(x), self.critic(x)\n",
                "\n",
                "class PPOAgent:\n",
                "    def __init__(self, cfg, drive_path):\n",
                "        for k, v in cfg.items():\n",
                "            setattr(self, k, v)\n",
                "        self.drive_path = drive_path\n",
                "        self.video_folder = f\"{drive_path}/videos\"\n",
                "        self.model_folder = f\"{drive_path}/models\"\n",
                "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "        self.env = self.make_env()\n",
                "        self.output_size = self.env.action_space.n\n",
                "        self.FIRE_ACTION = self.env.unwrapped.get_action_meanings().index(\"FIRE\")\n",
                "        self.agent = ActorCritic(self.hidden_layers, self.output_size).to(self.device)\n",
                "        self.optimizer = optim.Adam(self.agent.parameters(), lr=self.learning_rate)\n",
                "        self.best_reward = -float('inf')\n",
                "\n",
                "    def make_env(self, record_video=False, episode_num=0):\n",
                "        env = gym.make(\"ALE/Breakout-v5\", frameskip=4, repeat_action_probability=0.0,\n",
                "                      render_mode=\"rgb_array\" if record_video else None)\n",
                "        if record_video:\n",
                "            temp_folder = \"/content/temp_videos\"\n",
                "            os.makedirs(temp_folder, exist_ok=True)\n",
                "            env = RecordVideo(env, video_folder=temp_folder, episode_trigger=lambda x: True,\n",
                "                            name_prefix=f\"episode_{episode_num}\")\n",
                "        env = ResizeObservation(env, (84, 84))\n",
                "        env = GrayscaleObservation(env)\n",
                "        env = RescaleObservation(env, min_obs=0.0, max_obs=1.0)\n",
                "        env = FrameStackObservation(env, 4)\n",
                "        return env\n",
                "\n",
                "    def cal_advantage(self, rewards, values, dones, lam=0.95):\n",
                "        rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
                "        dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
                "        T = rewards.shape[0]\n",
                "        advantages = torch.zeros(T, dtype=torch.float32, device=self.device)\n",
                "        gae = 0.0\n",
                "        for t in reversed(range(T)):\n",
                "            next_value = 0.0 if t == T - 1 else values[t + 1]\n",
                "            next_non_terminal = 0.0 if t == T - 1 else 1.0 - dones[t + 1]\n",
                "            delta = rewards[t] + self.gamma * next_value * next_non_terminal - values[t]\n",
                "            gae = delta + self.gamma * lam * (1.0 - dones[t]) * gae\n",
                "            advantages[t] = gae\n",
                "        returns = advantages + values\n",
                "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
                "        return advantages, returns\n",
                "\n",
                "    def sample_trajectory(self):\n",
                "        states, actions, log_probs, values, rewards, dones = [], [], [], [], [], []\n",
                "        obs, _ = self.env.reset()\n",
                "        for _ in range(2):\n",
                "            obs, _, _, _, _ = self.env.step(self.FIRE_ACTION)\n",
                "        done, episode_reward, steps = False, 0.0, 0\n",
                "        self.agent.train()\n",
                "        while not done and steps < self.max_step:\n",
                "            state = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
                "            states.append(state)\n",
                "            logits, val = self.agent(state)\n",
                "            dist = Categorical(logits=logits)\n",
                "            action = dist.sample().squeeze()\n",
                "            actions.append(action)\n",
                "            log_probs.append(dist.log_prob(action))\n",
                "            values.append(val.squeeze())\n",
                "            obs, reward, terminated, truncated, _ = self.env.step(action.item())\n",
                "            done = terminated or truncated\n",
                "            episode_reward += reward\n",
                "            rewards.append(float(np.sign(reward)))\n",
                "            dones.append(done)\n",
                "            steps += 1\n",
                "        states = torch.cat(states)\n",
                "        actions = torch.stack(actions)\n",
                "        log_probs = torch.stack(log_probs)\n",
                "        values = torch.cat(values)\n",
                "        advantages, returns = self.cal_advantage(rewards, values, dones)\n",
                "        return episode_reward, states, actions, log_probs, advantages, returns\n",
                "\n",
                "    def update(self, states, actions, old_log_p, advantages, returns):\n",
                "        dataset = TensorDataset(states, actions.detach(), old_log_p.detach(), \n",
                "                               advantages.detach(), returns.detach())\n",
                "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
                "        total_p_loss, total_v_loss, n = 0, 0, 0\n",
                "        for _ in range(self.ppo_step):\n",
                "            for batch in loader:\n",
                "                b_states, b_actions, b_old_lp, b_adv, b_ret = [x.to(self.device) for x in batch]\n",
                "                logits, val = self.agent(b_states)\n",
                "                dist = Categorical(logits=logits)\n",
                "                new_lp = dist.log_prob(b_actions)\n",
                "                ratio = (new_lp - b_old_lp).exp()\n",
                "                surr1 = ratio * b_adv\n",
                "                surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * b_adv\n",
                "                p_loss = -torch.min(surr1, surr2).mean() - self.entropy_coeff * dist.entropy().mean()\n",
                "                v_loss = F.smooth_l1_loss(val.squeeze(), b_ret)\n",
                "                loss = p_loss + v_loss\n",
                "                self.optimizer.zero_grad()\n",
                "                loss.backward()\n",
                "                nn.utils.clip_grad_norm_(self.agent.parameters(), 0.5)\n",
                "                self.optimizer.step()\n",
                "                total_p_loss += p_loss.item()\n",
                "                total_v_loss += v_loss.item()\n",
                "                n += 1\n",
                "        return total_p_loss / max(1, n), total_v_loss / max(1, n)\n",
                "\n",
                "    def record_episode(self, ep_num):\n",
                "        temp_folder = \"/content/temp_videos\"\n",
                "        env = self.make_env(record_video=True, episode_num=ep_num)\n",
                "        obs, _ = env.reset()\n",
                "        for _ in range(2):\n",
                "            obs, _, _, _, _ = env.step(self.FIRE_ACTION)\n",
                "        done, ep_rew = False, 0\n",
                "        self.agent.eval()\n",
                "        while not done:\n",
                "            state = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
                "            with torch.no_grad():\n",
                "                logits, _ = self.agent(state)\n",
                "                action = logits.argmax(dim=-1).item()\n",
                "            obs, reward, terminated, truncated, _ = env.step(action)\n",
                "            done = terminated or truncated\n",
                "            ep_rew += reward\n",
                "        env.close()\n",
                "        for file in os.listdir(temp_folder):\n",
                "            if file.endswith('.mp4'):\n",
                "                shutil.move(os.path.join(temp_folder, file),\n",
                "                          os.path.join(self.video_folder, f\"score_{self.best_reward:.0f}_{file}\"))\n",
                "        self.agent.train()\n",
                "\n",
                "    def train(self):\n",
                "        returns, p_losses, v_losses = [], [], []\n",
                "        for ep in range(self.max_episode):\n",
                "            ep_ret, states, actions, old_lp, adv, rets = self.sample_trajectory()\n",
                "            p_loss, v_loss = self.update(states, actions, old_lp, adv, rets)\n",
                "            returns.append(ep_ret)\n",
                "            p_losses.append(p_loss)\n",
                "            v_losses.append(v_loss)\n",
                "            if ep_ret > self.best_reward:\n",
                "                self.best_reward = ep_ret\n",
                "                torch.save(self.agent.state_dict(), \n",
                "                          f\"{self.model_folder}/best_{self.best_reward:.0f}.pth\")\n",
                "                print(f\"\\nNew best: {self.best_reward:.0f}\")\n",
                "                self.record_episode(ep)\n",
                "            if ep % 10 == 0:\n",
                "                clear_output(wait=True)\n",
                "                avg100 = np.mean(returns[-100:])\n",
                "                avg30 = np.mean(returns[-30:]) if len(returns) >= 30 else avg100\n",
                "                print(f\"Ep {ep:4d} | Rew: {ep_ret:6.2f} | Avg30: {avg30:6.2f} | \"\n",
                "                      f\"Avg100: {avg100:6.2f} | Best: {self.best_reward:.0f}\")\n",
                "        return returns, p_losses, v_losses"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "train"
            },
            "source": [
                "## 4. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_code"
            },
            "outputs": [],
            "source": [
                "config = {\n",
                "    \"hidden_layers\": [256], \"gamma\": 0.99, \"epsilon\": 0.2,\n",
                "    \"entropy_coeff\": 0.01, \"learning_rate\": 2.5e-4,\n",
                "    \"max_step\": 27000, \"max_episode\": 10000,\n",
                "    \"batch_size\": 64, \"ppo_step\": 8,\n",
                "    \"N_Trials\": 100, \"reward_threshold\": 30.0,\n",
                "}\n",
                "\n",
                "agent = PPOAgent(config, DRIVE_PATH)\n",
                "print(\"Starting training...\\n\")\n",
                "returns, p_losses, v_losses = agent.train()\n",
                "torch.save(agent.agent.state_dict(), f\"{DRIVE_PATH}/models/final.pth\")\n",
                "print(\"\\nDone!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}